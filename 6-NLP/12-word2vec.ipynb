{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ae55c6",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "Word2Vec is a word embedding technique in natural language processing (NLP) that allows words to be represented as vectors in a continuous vector space. Researchers at Google developed word2Vec that maps words to high-dimensional vectors to capture the semantic relationships between words. It works on the principle that words with similar meanings should have similar vector representations.\n",
    "\n",
    "#### 1.  CBOW (Continuous Bag of Words)\n",
    "1. Data Preparation\n",
    "First, prepare your text corpus:\n",
    "\n",
    "* Tokenize the text into words\n",
    "* Build a vocabulary of unique words\n",
    "* Assign each word a unique index\n",
    "* Create training examples with context windows\n",
    "\n",
    "Example: For the sentence \"The cat sits on the mat\" with window size = 2:\n",
    "\n",
    "Context: [The, cat, on, the] → Target: sits\n",
    "Context: [cat, sits, the, mat] → Target: on\n",
    "\n",
    "2. Define the Architecture\n",
    "The CBOW model has three layers:\n",
    "\n",
    "* Input Layer: Takes context words as one-hot encoded vectors\n",
    "* Hidden Layer: Projects words into a lower-dimensional embedding space (no activation function)\n",
    "* Output Layer: Predicts the target word using softmax\n",
    "\n",
    "Key parameters:\n",
    "\n",
    "* Vocabulary size: V\n",
    "* Embedding dimension: N (typically 100-300)\n",
    "* Context window size: C (words on each side of target)\n",
    "\n",
    "3. Initialize Weight Matrices\n",
    "Create two weight matrices with random small values:\n",
    "\n",
    "* W (V × N): Input-to-hidden weights (this becomes our word embeddings)\n",
    "* W' (N × V): Hidden-to-output weights\n",
    "\n",
    "4. Forward Propagation\n",
    "For each training example:\n",
    "* Step 4a: Convert context words to one-hot vectors\n",
    "\n",
    "* Each context word becomes a V-dimensional vector with 1 at its index, 0 elsewhere\n",
    "\n",
    "* Step 4b: Look up embeddings\n",
    "\n",
    "* Multiply each one-hot vector by W to get embedding vectors\n",
    "This is equivalent to selecting rows from W\n",
    "\n",
    "* Step 4c: Average the context embeddings\n",
    "\n",
    "* Sum all context word embeddings and divide by the number of context words\n",
    "Result: h = (1/C) × Σ(embeddings of context words)\n",
    "\n",
    "* Step 4d: Compute output scores\n",
    "\n",
    "Multiply averaged embedding h by W': u = h × W'\n",
    "u is a V-dimensional vector of scores for each vocabulary word\n",
    "\n",
    "* Step 4e: Apply softmax\n",
    "\n",
    "Convert scores to probabilities: p(w) = exp(u_w) / Σ(exp(u_i))\n",
    "This gives probability distribution over all vocabulary words\n",
    "\n",
    "5. Calculate Loss\n",
    "Use cross-entropy loss:\n",
    "\n",
    "Loss = -log(p(target_word))\n",
    "The goal is to maximize the probability of the correct target word\n",
    "\n",
    "6. Backward Propagation\n",
    "Compute gradients and update weights:\n",
    "Step 6a: Calculate output error\n",
    "\n",
    "Error at output = predicted probabilities - actual target (one-hot)\n",
    "\n",
    "Step 6b: Compute gradients for W'\n",
    "\n",
    "∂Loss/∂W' = h^T × error\n",
    "\n",
    "Step 6c: Backpropagate to hidden layer\n",
    "\n",
    "error_hidden = W' × error\n",
    "\n",
    "Step 6d: Compute gradients for W\n",
    "\n",
    "Update embeddings for each context word\n",
    "∂Loss/∂W = (1/C) × error_hidden for each context word position\n",
    "\n",
    "7. Update Weights\n",
    "Using gradient descent (or variants like SGD, Adam):\n",
    "\n",
    "W = W - learning_rate × ∂Loss/∂W\n",
    "W' = W' - learning_rate × ∂Loss/∂W'\n",
    "\n",
    "8. Iterate\n",
    "Repeat steps 4-7 for all training examples across multiple epochs until convergence.\n",
    "Key Optimizations\n",
    "Hierarchical Softmax: Replaces expensive softmax with binary tree structure, reducing complexity from O(V) to O(log V)\n",
    "Negative Sampling: Instead of updating all words in vocabulary, only update the target word (positive sample) and a small number of random words (negative samples)\n",
    "Final Output\n",
    "After training, the W matrix contains the learned word embeddings. Each row represents a word's vector representation that captures semantic meaning based on the contexts where it appears.\n",
    "Words used in similar contexts will have similar vector representations, enabling operations like:\n",
    "\n",
    "Similarity: king ≈ queen\n",
    "Analogies: king - man + woman ≈ queen\n",
    "\n",
    "\n",
    "Step-by-Step Process of Skip-Gram Word2Vec Model\n",
    "Overview\n",
    "Skip-Gram is the inverse of CBOW. Instead of predicting a target word from context, it predicts context words given a target word. Skip-Gram generally works better for smaller datasets and rare words.\n",
    "Detailed Process\n",
    "1. Data Preparation\n",
    "Prepare your text corpus:\n",
    "\n",
    "Tokenize the text into words\n",
    "Build a vocabulary of unique words\n",
    "Assign each word a unique index\n",
    "Create training examples with context windows\n",
    "\n",
    "Example: For \"The cat sits on the mat\" with window size = 2:\n",
    "\n",
    "Target: sits → Context words: The, cat, on, the\n",
    "This creates 4 separate training pairs:\n",
    "\n",
    "(sits, The)\n",
    "(sits, cat)\n",
    "(sits, on)\n",
    "(sits, the)\n",
    "\n",
    "\n",
    "\n",
    "Key Difference from CBOW: Skip-Gram creates multiple training examples (one per context word) while CBOW creates one example per target.\n",
    "2. Define the Architecture\n",
    "The Skip-Gram model has three layers:\n",
    "\n",
    "Input Layer: Takes a single target word as one-hot encoded vector\n",
    "Hidden Layer: Projects the word into embedding space (no activation function)\n",
    "Output Layer: Predicts each context word independently using softmax\n",
    "\n",
    "Key parameters:\n",
    "\n",
    "Vocabulary size: V\n",
    "Embedding dimension: N (typically 100-300)\n",
    "Context window size: C (words on each side)\n",
    "\n",
    "3. Initialize Weight Matrices\n",
    "Create two weight matrices with random small values:\n",
    "\n",
    "W (V × N): Input-to-hidden weights (word embeddings)\n",
    "W' (N × V): Hidden-to-output weights (context embeddings)\n",
    "\n",
    "4. Forward Propagation\n",
    "For each training pair (target_word, context_word):\n",
    "Step 4a: Convert target word to one-hot vector\n",
    "\n",
    "Create V-dimensional vector with 1 at target word's index, 0 elsewhere\n",
    "Example: if \"sits\" is word index 5 in vocab of size 1000, create vector with 1 at position 5\n",
    "\n",
    "Step 4b: Look up target word embedding\n",
    "\n",
    "Multiply one-hot vector by W to get embedding: h = x^T × W\n",
    "This is equivalent to selecting the row from W corresponding to the target word\n",
    "Result: h is an N-dimensional vector (the word embedding)\n",
    "\n",
    "Step 4c: Compute output scores\n",
    "\n",
    "Multiply embedding h by W': u = h × W'\n",
    "u is a V-dimensional vector of scores for predicting each vocabulary word\n",
    "\n",
    "Step 4d: Apply softmax\n",
    "\n",
    "Convert scores to probabilities: p(context_word_i) = exp(u_i) / Σ(exp(u_j))\n",
    "This gives probability distribution over all possible context words\n",
    "\n",
    "5. Calculate Loss\n",
    "For each (target, context) pair:\n",
    "\n",
    "Loss = -log(p(context_word | target_word))\n",
    "The goal is to maximize probability of the actual context word\n",
    "\n",
    "Total loss for one target word:\n",
    "\n",
    "Sum losses across all context words in the window\n",
    "Loss_total = -Σ log(p(context_word_i | target_word))\n",
    "\n",
    "6. Backward Propagation\n",
    "Compute gradients and update weights:\n",
    "Step 6a: Calculate output error\n",
    "\n",
    "error = predicted probabilities - actual context word (one-hot)\n",
    "This is a V-dimensional vector\n",
    "\n",
    "Step 6b: Compute gradients for W'\n",
    "\n",
    "∂Loss/∂W' = h^T × error\n",
    "This updates the context word representations\n",
    "\n",
    "Step 6c: Backpropagate to hidden layer\n",
    "\n",
    "error_hidden = W' × error\n",
    "This is an N-dimensional vector\n",
    "\n",
    "Step 6d: Compute gradients for W\n",
    "\n",
    "∂Loss/∂W = x × error_hidden^T\n",
    "This updates only the embedding of the target word (one row of W)\n",
    "\n",
    "7. Update Weights\n",
    "Using gradient descent (or variants):\n",
    "\n",
    "W = W - learning_rate × ∂Loss/∂W\n",
    "W' = W' - learning_rate × ∂Loss/∂W'\n",
    "\n",
    "Important: Only the row corresponding to the target word in W gets updated in each iteration.\n",
    "8. Iterate\n",
    "Repeat steps 4-7 for all training pairs across multiple epochs until convergence.\n",
    "Training Example Walkthrough\n",
    "Sentence: \"The cat sits on the mat\" (window size = 2)\n",
    "All training pairs generated:\n",
    "\n",
    "(The, cat) - \"The\" predicts \"cat\"\n",
    "(cat, The), (cat, sits) - \"cat\" predicts \"The\" and \"sits\"\n",
    "(sits, The), (sits, cat), (sits, on), (sits, the) - \"sits\" predicts 4 context words\n",
    "(on, cat), (on, sits), (on, the), (on, mat) - \"on\" predicts 4 context words\n",
    "(the, sits), (the, on), (the, mat) - \"the\" predicts 3 context words\n",
    "(mat, on), (mat, the) - \"mat\" predicts 2 context words\n",
    "\n",
    "Each pair is trained separately.\n",
    "Key Optimizations\n",
    "Negative Sampling (Most Common)\n",
    "Instead of using expensive softmax over entire vocabulary:\n",
    "\n",
    "Update the actual context word (positive sample)\n",
    "Update k random words that are NOT in context (negative samples, typically k=5-20)\n",
    "Loss becomes: log(σ(u_pos)) + Σ log(σ(-u_neg))\n",
    "This reduces complexity from O(V) to O(k)\n",
    "\n",
    "Hierarchical Softmax\n",
    "\n",
    "Organizes vocabulary in binary tree\n",
    "Reduces complexity from O(V) to O(log V)\n",
    "Each word is a leaf; path from root determines probability\n",
    "\n",
    "Subsampling Frequent Words\n",
    "\n",
    "Frequent words (like \"the\", \"a\") are randomly discarded during training\n",
    "Probability of keeping word w: P(w) = √(t/f(w))\n",
    "This balances training and improves rare word representations\n",
    "\n",
    "Skip-Gram vs CBOW Comparison\n",
    "AspectSkip-GramCBOWPredictionContext from targetTarget from contextTraining pairsMultiple per windowOne per windowTraining timeSlower (more examples)FasterPerformanceBetter on small data, rare wordsBetter on large data, frequent wordsEmbedding qualityHigher quality for infrequent wordsSmoother embeddings overall\n",
    "Final Output\n",
    "After training, the W matrix contains the learned word embeddings. Each row is a dense vector representation of a word that captures:\n",
    "\n",
    "Semantic meaning\n",
    "Syntactic patterns\n",
    "Contextual relationships\n",
    "\n",
    "Words appearing in similar contexts will have similar vectors, enabling:\n",
    "\n",
    "Similarity: dog ≈ cat\n",
    "Analogies: paris - france + germany ≈ berlin\n",
    "Clustering: Related words cluster together in vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a3faaa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
