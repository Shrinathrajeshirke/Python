--- vanishing gradients --- 
- when error signals travel backward through many layers, they become smaller and smaller until they essentially disappear.
- This means:
-- Early layers (near input) learn extremely slowly or not at all
-- Later layers (near output) learn normally
-- The network can't learn deep, complex patterns
- backpropagation uses the chain rule - we multiply gradients as we move backward through layers:
Gradient at Layer 1 = Gradient at Output × Weight₄ × Weight₃ × Weight₂ × ..
- When we use certain activation functions like sigmoid or tanh, their derivatives (slopes) are always less than 1:
-- Sigmoid function:
Output range: 0 to 1
Maximum derivative: 0.25 (at input = 0)
Typical derivative: 0.1 to 0.2
-- Tanh function:
Output range: -1 to 1
Maximum derivative: 1.0 (at input = 0)
Typical derivative: 0.1 to 0.4
- When you multiply many small numbers together, the result becomes extremely tiny.

-- Solutions to Vanishing Gradients
Solution 1: ReLU Activation Function (Most Important!)
- ReLU (Rectified Linear Unit) revolutionized deep learning:
- ReLU(x) = max(0, x)
- If x > 0: output = x,  derivative = 1
- If x ≤ 0: output = 0,  derivative = 0
Why it helps:
- Derivative is either 0 or 1 (not a small fraction!)
- No saturation for positive values
- Gradients pass through unchanged when active
-- ReLU variants that improve further:
- Leaky ReLU: Small slope for negative values (derivative = 0.01) instead of complete zero
- PReLU: Learns the negative slope during training
- ELU, GELU: Smooth alternatives that avoid "dying ReLU" problem
Solution 2: Proper Weight Initialization
- Xavier/Glorot Initialization (for sigmoid/tanh):
Initialize weights with variance = 1/n_inputs
- He Initialization (for ReLU):
Initialize weights with variance = 2/n_inputs
Solution 3: Batch Normalization
- Normalize the inputs to each layer during training:
For each mini-batch:
  1. Calculate mean and variance
  2. Normalize: (x - mean) / sqrt(variance)
  3. Scale and shift with learned parameters
Benefits:
- Keeps activations in a reasonable range
- Reduces internal covariate shift
- Acts as a regularizer
- Allows higher learning rates
Solution 4: Residual Connections (ResNets)
- Add "skip connections" that bypass layers:
- Why it helps:
- Gradients can flow directly through shortcuts
- Network learns residual (difference) instead of full transformation
- Enables training of 100+ layer networks
Solution 5: LSTM and GRU (For Recurrent Networks)
- Special architectures for sequential data with gates that control information flow:
LSTM gates:
Forget gate: What to discard from memory
Input gate: What new information to store
Output gate: What to output
- These gates help gradients flow across many time steps without vanishing.
Solution 6: Gradient Clipping
- For exploding gradients, simply cap the gradient magnitude:
if gradient_norm > threshold:
    gradient = gradient × (threshold / gradient_norm)
Prevents catastrophic updates from destroying learning.
Solution 7: Layer Normalization & Other Techniques
Layer Normalization: Normalize across features (good for RNNs)
Smaller Networks: Sometimes simpler is better
Better Optimizers: Adam, RMSprop adapt learning rates per parameter